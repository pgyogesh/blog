---
layout: single
excerpt: "Python utility to process Greenplum objects in bulk and in parallel"
comments: true
header:
  overlay_image: https://source.unsplash.com/random/1200x400?nature,technology,city
  overlay_filter: 0.5
title:  "GPOPB: Greenplum Object Processing in Bulk"
date:   2018-07-25 10:10:00 +0800
categories: Greenplum Python
tags: greenplum python postgresql bulk-processing parallel-processing
---

## Introduction

I have developed a python utility to process the Greenplum in objects in bulk and in parallel. This utility can perform any task that we can do using SQL queries like VACUUM, VACUUM FULL, ANALYZE, REINDEX, changing permissions and many.

This utility uses 3 modules such as process, objects and configuration module. You can find these modules in `config` directory.

### Process module

In this module, We can define what task we have to perform such VACUUM, REINDEX or any other task you would like to do.

### Configuration Module

In this module, We have specify the database name and maximum parallel threads

### Object module

In this module, We can fetch the database objects using SQL query. If fetching a set of objects using SQL query is not possible, We can mention all objects in single file (one object per line) and `--ad-hoc` option to specify to file name.

For Example:

`python gpopb.py --ad-hoc <path/to/file>`

## Installation

you can clone GPOPB using below command start using it.

`git clone https://github.com/pgyogesh/GPOPB.git`

## Utility

### gpopb.py

{% highlight python linenos %}

from multiprocessing import Pool, Value
from config import configuration
from config import objects
from config import process
from gppylib import gplog
import optparse

# Logger
logger = gplog.get_default_logger()
gplog.setup_tool_logging("gpopb", '', "gpadmin")

# Command line option parser
parser = optparse.OptionParser()
parser.add_option('--ad-hoc', dest='adhoc', action='store', help="Specify filename for list of objects")
options, args = parser.parse_args()

max_processes = configuration.MAX_THREADS

logger.info("Getting list of objects:")
db_objects = []
if options.adhoc:
        for line in open(options.adhoc,'r'):
                db_objects.append(line.rstrip('\n'))
else:
        db_objects = objects.get_objects()

logger.info("Objects to be processed: %s" %',\n'.join(db_objects))

def init(args):
    ''' store the counter for later use '''
    global counter
    counter = args

pool = Pool(initializer=init, initargs=(process.counter, ), processes=max_processes)
pool.map(process.task, db_objects)

pool.close()
pool.join()
logger.info("Object processing completed")

{% endhighlight %}

This the main program file we have to run.

#### Help:

{% highlight python linenos %}

[gpadmin@mdw GPOPB]$ python gpopb.py --help
Usage: gpopb.py [options]

Options:
  -h, --help      show this help message and exit
  --ad-hoc=ADHOC  Specify filename for list of objects

{% endhighlight %}

`--ad-hoc` option allows us to specify the file which contains list of objects.

### process.py

{% highlight python linenos %}

from multiprocessing import Value
from pygresql.pg import DB
from gppylib import gplog
import configuration
logger = gplog.get_default_logger()
gplog.setup_tool_logging("gpopb", '', "gpadmin")

database = configuration.DATABASE
counter = Value('i', 0)
def task(db_object):
        global counter
        con = DB(dbname = database)
        con.query("vacuum %s" %(db_object))
        con.close()
        with counter.get_lock():
                counter.value += 1
        logger.info(str(counter.value) + " objects completed")

if __name__ == '__main__':
        print("this program should not be running alone :P")

{% endhighlight %}

The above process module is example for VACUUM task (Line 13). Similar way can have write query for other tasks. See few examples below:

{% highlight SQL linenos %}

-- ANALYZE
con.query("ANALYZE %s" %(db_object))

-- VACUUM ANALYZE
con.query("VACUUM ANALYZE %s" %(db_object))

-- CREATE TABLE AS SELECT -- Alternative for VACUUM FULL
con.query("CREATE TABLE %s_temp AS SELECT * FROM %s" %(db_object,db_object))
con.query("ALTER TABLE %s RENAME TO %s_hold" %(db_object))
con.query("ALTER TABLE %s_temp RENAME TO %s" %(db_object,db_object))
con.query("DROP TABLE %s_hold" %s(db_object))

{% endhighlight %}

### objects.py

{% highlight python linenos %}

from multiprocessing import Value
from pygresql.pg import DB
from gppylib import gplog
import configuration
logger = gplog.get_default_logger()
gplog.setup_tool_logging("gpopb", '', "gpadmin")

database = configuration.DATABASE
counter = Value('i', 0)
def task(db_object):
        global counter
        con = DB(dbname = database)
        con.query("vacuum %s" %(db_object))
        con.close()
        with counter.get_lock():
                counter.value += 1
        logger.info(str(counter.value) + " objects completed")

if __name__ == '__main__':
        print("this program should not be running alone :P")
[gpadmin@mdw GPOPB]$ cat config/objects.py
import configuration
from pygresql.pg import DB

def get_objects():
        database = configuration.DATABASE
        con = DB(dbname=database)
        tables = con.query("select schemaname||'.'||tablename as tablename from pg_tables where schemaname='public'")
        tabledict = tables.dictresult()
        tablelist = []
        con.close()
        for dict in tabledict:
                tablelist.append(dict.get('tablename')) # You should replace the 'tablename' with column name from your SQL query in tables variable
        return tablelist

if __name__ == '__main__':
        print(get_objects())

{% endhighlight %}

This module is to fetch the database objects like tables and indexes (line 28). 

### configuration.py

{% highlight python linenos %}

MAX_THREADS = 8
DATABASE = 'gpadmin'

{% endhighlight %}

In configuration module, We can configure the maximum parallel processes and database name.

Let me know if you have any issues, suggestions in comment box below. Also, I would like to see what you achieve with this. Thanks :smile:
